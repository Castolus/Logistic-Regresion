# -*- coding: utf-8 -*-
"""Regression_HousePrice

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cMqZB234obgK3p2ciJaVVzebAgfzlFW9

#**FINAL PROJECT FOR MODULE 1**#
Aim of the project:

The aim of this project is to be able to perform an exploratory analysis of a dataset, and to perform a regression model to predict house prices.

#1. DATA UPLOAD #
"""

# Import the file locally
from google.colab import files
uploaded = files.upload()

"""#2. EXPLORATORY PHASE / PRE-PROCESSING

We analyse the characteristics of the Dataset in order to draw conclusions beforehand.

##2.1. DATASET SCANNING

This section discusses the general characteristics of the dataset, in order to understand how it is composed.
"""

import pandas as pd
import numpy as np
house_data = pd.read_csv('data.csv') #Create varaible with dataset
house_data #Explore Dataset

house_data.describe().transpose()

house_data.shape

house_data.columns

# Ask variable rate
house_data.dtypes

#house_data['price']=house_data['price'].apply(np.int64)

house_data.isnull().sum()

# Fill null values with the mean
house_data.fillna(house_data.mean())

# Fill in null values with the most frequent value
house_data['bedrooms'] = house_data['bedrooms'].fillna(house_data['bedrooms'].mode()[0])

# Query numerical variables of the dataset
house_data.describe(include=np.number)

# Query numerical variables of the dataset
house_data.describe(include=object)

# Eliminate the variable "date" because it is not relevant.
house_data.drop(columns='date',inplace=True)

# Identify numerical variables and categorical variables
num_var=house_data.select_dtypes(include=np.number).columns.to_list()
cat_var=house_data.select_dtypes(include=object).columns.to_list()

num_var

cat_var

"""## 2.2 Visual analysis of data

We visualise the different variables of the Dataset by means of graphs.
"""

# Import visualisation libraries 
import matplotlib.pyplot as plp
import seaborn as sns

# Make a copy of the dataset to keep the original one
house_data1=house_data

house_data1

# Analyze the distribution of each variable
i=1
plp.figure(figsize=(15,20))
for j in num_var:
    plp.subplot(5,3,i)
    sns.boxplot(data=house_data1,x=j)
    i+=1

# Outlier elimination
def outlier_tret(x):
    upper=x.quantile(0.98)
    lower=x.quantile(0.2)
    x=np.where(x>upper,upper,x)
    x=np.where(x<lower,lower,x)
    return x

house_data1[num_var]=house_data1[num_var].apply(lambda x: outlier_tret(x))

# Re-analyze the distribution of each variable after Outlier elimination.
i=1
plp.figure(figsize=(15,20))
for j in num_var:
    plp.subplot(5,3,i)
    sns.boxplot(data=house_data1,x=j)
    i+=1

# Regplot visualisation
sns.set(style="darkgrid")
sns.regplot(x=house_data1['bedrooms'], y= house_data1["price"])
plp.show()

"""With the exception of 2 cases to be analysed, the highest prices are found in 4 and 5-bedroom properties, although they then fall again. That is to say, up to a maximum of 5 bedrooms, it could be said that the value of the property rises. """

# Regplot visualisation
sns.set(style="darkgrid")
sns.scatterplot(x=house_data1['bedrooms'], y= house_data1["price"], hue=house_data1['view'])
plp.show()

"""If we plot according to the variable "View" we obtain a graph very similar to the previous one."""

# Regplot visualisation
sns.set(style="darkgrid")
sns.scatterplot(x=house_data1['sqft_living'], y= house_data1["price"])
plp.show()

# Regplot visualisation
sns.set(style="darkgrid")
sns.scatterplot(x=house_data1['yr_built'], y= house_data1["price"])
plp.show()

"""The year of construction does not seem to be relevant for the price of the property either."""

# Histogram display
house_data1.hist(figsize=(15,20))
plp.show()

# The variable "waterfront" always has the same value, so we remove it from the dataset.
house_data1=house_data1.drop(columns='waterfront')

# Analysis of the different values of categorical variables
house_data1[cat_var].nunique()

# We leave only the "city" variable
house_data1.drop(columns=['street','statezip','country'],inplace=True)

plp.figure(figsize=(15, 10))
correlacion = house_data1.corr()
sns.heatmap(correlacion, annot=True);

"""We observe a strong and obvious correlation between the square metres of the dwelling and the square metres of the upper floor of the house. As well as between the number of bathrooms and the square metres of the house and the upper floor. The larger the living area, the greater the number of bathrooms. Appart of this obvious correlations, there aren´t more strong correlations to mention.

"""

# We order the correlation of variables, with respect to price.
corr=house_data1.corr()["price"].sort_values(ascending=False)

# Visualization of the relationship of the numerical variables with the "price" variable.
plp.figure(figsize=(10,5))
corr.plot(kind='bar',color='black');

# Visualization of the relationship between the categorical variable "city" and the variable "price".
plp.figure(figsize=(15,20))
sns.barplot(data=house_data1,y='city',x='price',orient="h3");

"""# PHASE 3: TRAINING AND VALIDATION PHASE

##HYPOTHESIS 1: Using only numerical variables

During the exercise, we will run different models applying different data treatments to see how these affect the outcome on the metrics.
Since the models only accept numerical variables, we will eliminate all variables that are not numerical.
"""

house_data_hip1=house_data1

x_hip1 = house_data_hip1.drop(columns=['city','price'],axis=1) # Independent variables
y_hip1 = house_data_hip1['price'] # Dependent variable

house_data_hip1.head()

# We scale the variables to make it more standard.
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

x_hip1 = scaler.fit_transform(x_hip1)
#sklearn's preprocessing algorithms are prepared to convert matrices so we have to make a transformation of our variable y
# since it is a variable of type Series
# for this we do a .to_numpy() which converts the series into an array and then we do reshape (-1,1) which transforms an array of 1xn into an array of nx1.
y_hip1 = scaler.fit_transform(y_hip1.to_numpy().reshape(-1,1))
# Volvemos a transformar nuestra variable en un array de 1xn
y_hipot1=y_hip1.reshape(1,-1)[0]

"""### HYPOTHESIS 1a: Using a 70/30 dataset 

"""

# We prepare the data for training and for validation.
from sklearn.model_selection import train_test_split 
x_h1_train_30 , x_h1_test_30, y_h1_train_30, y_h1_test_30 = train_test_split(x_hip1, y_hip1, test_size=0.3, random_state=43) #Se utiliza un conjunto de 30% para test

"""### HYPOTHESIS 1b: Using an 80/20 dataset 

"""

# We prepare the data for training and for validation.
from sklearn.model_selection import train_test_split 
x_h1_train_20 , x_h1_test_20, y_h1_train_20, y_h1_test_20 = train_test_split(x_hip1, y_hip1, test_size=0.2, random_state=43) #Se utiliza un conjunto de 20% para test

"""I divide Hypothesis 1 into 2 sections: in the first one I use 70% of the data for training and 30% for the test. In the second I use 80% for training and 20% for the test.

I think it is better to have two different options to be able to draw more conclusions.
"""

from sklearn.linear_model import LinearRegression 

#Create the Line Regressions we are going to use with 20% and 30%.
reg_lineal_20 = LinearRegression() 
reg_lineal_30 = LinearRegression()

reg_lineal_20.fit(x_h1_train_20, y_h1_train_20) # linear regression for 20% of test
reg_lineal_30.fit(x_h1_train_30, y_h1_train_30) # linear regression for 30% of test

from sklearn.metrics import mean_squared_error # Import metrics 

pred_train_20 = reg_lineal_20.predict(x_h1_train_20) # Predict values for training data (80% of training)
pred_train_30 = reg_lineal_30.predict(x_h1_train_30) # Predict values for training data (70% of training)

pred_test_20 = reg_lineal_20.predict(x_h1_test_20) # Predict values for validation data (20% validation)
pred_test_30 = reg_lineal_30.predict(x_h1_test_30) # Predict values for validation data (30% validation)


#Calculate the quadratic error for the predictions made.
mse_h1_train_20 = mean_squared_error(y_true = y_h1_train_20, y_pred = pred_train_20)
mse_h1_test_20 = mean_squared_error(y_true = y_h1_test_20, y_pred = pred_test_20)
mse_h1_train_30 = mean_squared_error(y_true = y_h1_train_30, y_pred = pred_train_30)
mse_h1_test_30 = mean_squared_error(y_true = y_h1_test_30, y_pred = pred_test_30)

# Show the results
print('Error cuadratico medio (MSE) Train (HIPO 1a: 0.2) = ' + str(mse_h1_train_20))
print('Error cuadratico medio (MSE) Train (HIPO 1b: 0.3) = ' + str(mse_h1_train_30))

print('Error cuadratico medio (MSE) Test  (HIPO 1a: 0.2) = ' + str(mse_h1_test_20))
print('Error cuadratico medio (MSE) Test  (HIPO 1b: 0.3) = ' + str(mse_h1_test_30))

"""It is observed that in the 80/20 model we have better results. We will use this model."""

# VALIDATION PHASE

acc=reg_lineal_20.score(x_h1_train_20,y_h1_train_20)
print('Accuracy= '+str(acc))

# Import the mean squared error (MSE) calculation
from sklearn.metrics import mean_squared_error

# Predict values and for the data used in training
pred_training = reg_lineal_20.predict(x_h1_train_20)

# calculamos el Error Cuadrático Medio (MSE = Mean Squared Error)
mse_h1_train_20 = mean_squared_error(y_true = y_h1_train_20, y_pred = pred_training)
print('Error Cuadrático Medio (MSE) HIPO 1 TRAIN= ' + str(mse_h1_train_20))

# predecimos los valores y para los datos usados en el entrenamiento
pred_training = reg_lineal_20.predict(x_h1_test_20)

# calculamos el Error Cuadrático Medio (MSE = Mean Squared Error)
mse_h1_test_20 = mean_squared_error(y_true = y_h1_test_20, y_pred = pred_training)
print('Error Cuadrático Medio (MSE) HIPO 1 TEST= ' + str(mse_h1_test_20))

"""The metrics obtained are not very good, so we decided to move forward with the second hypothesis.

##HYPOTHESIS 2: Transform the variables 'city' and 'statezip' into numeric variables.
Transform the String variables into INT variables.
"""

# Prepare the dataset of hypothesis 2
house_data_hip2 = pd.get_dummies(data = house_data1, prefix = 'OHE', prefix_sep='_',
               columns = ['city'],
               drop_first =True,
               dtype='int8')

# Check First Lines of our dataSet
house_data_hip2.head()

x_hip2 = house_data_hip2.drop('price', axis=1)
y_hip2 = house_data_hip2['price']

# Scaling variables to make it more standardized
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

x_hip2 = scaler.fit_transform(x_hip2)
# Sklearn's preprocessing algorithms are prepared to convert matrices so we have to make a transformation of our variable y
# since it is a variable of type Series
# for this we do a .to_numpy() which converts the series into an array and then we do reshape (-1,1) which transforms an array of 1xn into an array of nx1.
y_hip2 = scaler.fit_transform(y_hip2.to_numpy().reshape(-1,1))
# Volvemos a transformar nuestra variable en un array de 1xn
y_hip2=y_hip2.reshape(1,-1)[0]

# Prepare the training and validation data
x_h2_train_20 , x_h2_test_20, y_h2_train_20, y_h2_test_20 = train_test_split(x_hip2, y_hip2, test_size=0.2, random_state=43)
reg_lineal_20 = LinearRegression() # Linear Regression

reg_lineal_20.fit(x_h2_train_20, y_h2_train_20)

# VALIDATION PHASE
acc2=reg_lineal_20.score(x_h2_train_20,y_h2_train_20)
print('Accuracy 1= '+str(acc))
print('Accuracy 2= '+str(acc2))

# Predict values and for the data used in training
pred_training = reg_lineal_20.predict(x_h2_train_20)

# calculamos el Error Cuadrático Medio (MSE = Mean Squared Error)
mse_h2_train_20 = mean_squared_error(y_true = y_h2_train_20, y_pred = pred_training)
print('Error Cuadrático Medio (MSE) HIPO 1 TRAIN= ' + str(mse_h1_train_20))
print('Error Cuadrático Medio (MSE) HIPO 2 TRAIN= ' + str(mse_h2_train_20))

# predecimos los valores y para los datos usados en el entrenamiento
pred_training = reg_lineal_20.predict(x_h2_test_20)

# calculamos el Error Cuadrático Medio (MSE = Mean Squared Error)
mse_h2_test_20 = mean_squared_error(y_true = y_h2_test_20, y_pred = pred_training)
print('Error Cuadrático Medio (MSE) HIPO 1 TEST= ' + str(mse_h1_test_20))
print('Error Cuadrático Medio (MSE) HIPO 2 TEST= ' + str(mse_h2_test_20))

"""There is some improvement, but still not good values.

##HYPOTHESIS 3: Using MaxAbsScaler
"""

house_data_hip3 = pd.get_dummies(data = house_data1, prefix = 'OHE', prefix_sep='_',
               columns = ['city'],
               drop_first =True,
               dtype='int8')

from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler()
house_data_hip3[["price","sqft_living","sqft_above","sqft_basement",'sqft_lot','condition','yr_built','yr_renovated']] = scaler.fit_transform(house_data_hip3[["price","sqft_living","sqft_above","sqft_basement",'sqft_lot','condition','yr_built','yr_renovated']] )

x_hip3_20 = house_data_hip3.drop(columns=['price'],axis=1) 
y_hip3_20 = house_data_hip3['price'] #  variable depnediente

# preparamos train data y test data
x_hip3_train_20, x_hip3_test_20, y_hip3_train_20, y_hip3_test_20 = train_test_split(x_hip3_20, y_hip3_20, test_size=0.20, random_state=43)
#Regresion lineal
regresion_lineal=LinearRegression()
regresion_lineal.fit(x_hip3_train_20, y_hip3_train_20)

acc3=regresion_lineal.score(x_hip3_train_20,y_hip3_train_20)
print('Accuracy 1= '+str(acc))
print('Accuracy 2= '+str(acc2))
print('Accuracy 3= '+str(acc3))

# predecimos los valores y para los datos usados en el entrenamiento
prediccion_entrenamiento = regresion_lineal.predict(x_hip3_train_20)

# calculamos el Error Cuadrático Medio (MSE = Mean Squared Error)
mse_h3_train_20 = mean_squared_error(y_true = y_hip3_train_20, y_pred = prediccion_entrenamiento)
print('Error Cuadrático Medio (MSE) HIPO 1 TRAIN= ' + str(mse_h1_train_20))
print('Error Cuadrático Medio (MSE) HIPO 2 TRAIN= ' + str(mse_h2_train_20))
print('Error Cuadrático Medio (MSE) HIPO 3 TRAIN= ' + str(mse_h3_train_20))

# predecimos los valores y para los datos usados en el entrenamiento
prediccion_entrenamiento = regresion_lineal.predict(x_hip3_test_20)

# calculamos el Error Cuadrático Medio (MSE = Mean Squared Error)
mse_h3_test_20 = mean_squared_error(y_true = y_hip3_test_20, y_pred = prediccion_entrenamiento)
print('Error Cuadrático Medio (MSE) HIPO 1 TEST= ' + str(mse_h1_test_20))
print('Error Cuadrático Medio (MSE) HIPO 2 TEST= ' + str(mse_h2_test_20))
print('Error Cuadrático Medio (MSE) HIPO 3 TEST= ' + str(mse_h3_test_20))

"""##HYPOTHESIS 4: Normalisation of the dependent (target) variable using the MinMaxScaler scaler
For this experiment, the MinMaxScaler will be used to determine how the model is affected by using another scaler.
"""

from sklearn.preprocessing import MinMaxScaler # Import the scaler to be used: MinMaxScaler
scaler = MinMaxScaler() # Create the scaler

house_data_hip4 = pd.get_dummies(data = house_data1, prefix = 'OHE', prefix_sep='_',
               columns = ['city'],
               drop_first =True,
               dtype='int8')

house_data_hip4[['price']] = scaler.fit_transform(house_data_hip4[['price']])

x_hip4 = house_data_hip4.drop('price', axis=1)
y_hip4 = house_data_hip4['price']

# Prepare the data to be used
x_h4_train_20 , x_h4_test_20, y_h4_train_20, y_h4_test_20 = train_test_split(x_hip4, y_hip4, test_size=0.2, random_state=43)
reg_lineal_20 = LinearRegression()
reg_lineal_20.fit(x_h4_train_20, y_h4_train_20)

#Validation phase
pred_train_20 = reg_lineal_20.predict(x_h4_train_20) # Predict values for training data (80% of training)
pred_test_20 = reg_lineal_20.predict(x_h4_test_20) # Predict values for validation data (80% validation)

#Calculate the quadratic error for the predictions made.
mse_h4_train_20 = mean_squared_error(y_true = y_h4_train_20, y_pred = pred_train_20)
mse_h4_test_20 = mean_squared_error(y_true = y_h4_test_20, y_pred = pred_test_20)

# Presenting the results
print('Error cuadratico medio (MSE) Train (HIPO 1: 0.2) = ' + str(mse_h1_train_20))
print('Error cuadratico medio (MSE) Train (HIPO 2: 0.2) = ' + str(mse_h2_train_20))
print('Error cuadratico medio (MSE) Train (HIPO 3: 0.2) = ' + str(mse_h3_train_20))
print('Error cuadratico medio (MSE) Train (HIPO 4: 0.2) = ' + str(mse_h4_train_20))
print('')
print('Error cuadratico medio (MSE) Test  (HIPO 1: 0.2) = ' + str(mse_h1_test_20))
print('Error cuadratico medio (MSE) Test  (HIPO 2: 0.2) = ' + str(mse_h2_test_20))
print('Error cuadratico medio (MSE) Test  (HIPO 3: 0.2) = ' + str(mse_h3_test_20))
print('Error cuadratico medio (MSE) Test  (HIPO 4: 0.2) = ' + str(mse_h4_test_20))

"""According to the data obtained, no considerable differences were found between the use of the two scalers.
Even though, we observe a minimal deterioration of the result.

----------------------------------------------------------------------------------------

# PHASE 4: Evaluation and analysis of the results

Based on the results obtained, it can be seen that the best results are presented from Hypothesis 3, where we make use of the Scaler "maxAbsScaler".

The geographical location of housing is clearly a very important factor affecting house prices. We deleted te Variables "Statezip", "Street" and "Country" since they are irrelevant. I believe that "City" the only Geographical Variable usefull. 

Also, the variable Date is not relevant because, as can be seen in the Dataset, it does not vary in just one day and confuses the result of our Modell.

Apart from all that, the square metres of the house, year of renovation, number of rooms, views... are factors that also affect the price, although not as much as the geographical location of the property.

The Accuray of the first Training Test stays around 53%, while the Accuracy for the other 2 Hypothesis is around 66%. 

Nevertheless, it is after using a Scaler when I obteined good results both in Training and Test.
"""